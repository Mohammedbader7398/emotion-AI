{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "DATA_DIR = r\"archive\"  \n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "EPOCHS = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16108 files belonging to 8 classes.\n",
      "Using 12887 files for training.\n",
      "Found 16108 files belonging to 8 classes.\n",
      "Using 3221 files for validation.\n",
      "Found 14518 files belonging to 8 classes.\n",
      "Classes: ['anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Num classes: 8\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(DATA_DIR, \"train\")\n",
    "test_dir  = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Num classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.08),\n",
    "    layers.RandomZoom(0.10),\n",
    "    layers.RandomContrast(0.10),\n",
    "], name=\"augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " augmentation (Sequential)   (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional  (None, 3, 3, 1280)        4049571   \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 10248     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4059819 (15.49 MB)\n",
      "Trainable params: 10248 (40.03 KB)\n",
      "Non-trainable params: 4049571 (15.45 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base = tf.keras.applications.EfficientNetB0(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_aug(inputs)\n",
    "x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"affectnet_best.keras\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\"),\n",
    "    keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_loss\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From e:\\emotion-ai\\fer_env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\emotion-ai\\fer_env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1799\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1800\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1801\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1804\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1805\u001b[39m ):\n\u001b[32m   1806\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1807\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1808\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1809\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    829\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    835\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:905\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[32m    902\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m   bound_args = \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.bind(\n\u001b[32m    910\u001b[39m       *args, **kwds\n\u001b[32m    911\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1319\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1321\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1322\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1323\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1324\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1325\u001b[39m     args,\n\u001b[32m   1326\u001b[39m     possible_gradient_type,\n\u001b[32m   1327\u001b[39m     executing_eagerly)\n\u001b[32m   1328\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1484\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1494\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1495\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1496\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1501\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\emotion-ai\\fer_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "202/202 - 119s - loss: 1.4029 - accuracy: 0.4718 - val_loss: 1.3485 - val_accuracy: 0.4859 - lr: 3.0000e-05 - 119s/epoch - 590ms/step\n",
      "Epoch 2/15\n",
      "202/202 - 108s - loss: 1.2816 - accuracy: 0.5173 - val_loss: 1.2705 - val_accuracy: 0.5172 - lr: 3.0000e-05 - 108s/epoch - 534ms/step\n",
      "Epoch 3/15\n",
      "202/202 - 118s - loss: 1.2136 - accuracy: 0.5435 - val_loss: 1.2182 - val_accuracy: 0.5359 - lr: 3.0000e-05 - 118s/epoch - 585ms/step\n",
      "Epoch 4/15\n",
      "202/202 - 110s - loss: 1.1678 - accuracy: 0.5569 - val_loss: 1.1856 - val_accuracy: 0.5477 - lr: 3.0000e-05 - 110s/epoch - 543ms/step\n",
      "Epoch 5/15\n",
      "202/202 - 107s - loss: 1.1189 - accuracy: 0.5772 - val_loss: 1.1577 - val_accuracy: 0.5591 - lr: 3.0000e-05 - 107s/epoch - 530ms/step\n",
      "Epoch 6/15\n",
      "202/202 - 102s - loss: 1.0690 - accuracy: 0.5991 - val_loss: 1.1383 - val_accuracy: 0.5666 - lr: 3.0000e-05 - 102s/epoch - 504ms/step\n",
      "Epoch 7/15\n",
      "202/202 - 101s - loss: 1.0367 - accuracy: 0.6113 - val_loss: 1.1219 - val_accuracy: 0.5725 - lr: 3.0000e-05 - 101s/epoch - 499ms/step\n",
      "Epoch 8/15\n",
      "202/202 - 101s - loss: 1.0011 - accuracy: 0.6263 - val_loss: 1.1167 - val_accuracy: 0.5790 - lr: 3.0000e-05 - 101s/epoch - 498ms/step\n",
      "Epoch 9/15\n",
      "202/202 - 103s - loss: 0.9704 - accuracy: 0.6358 - val_loss: 1.0969 - val_accuracy: 0.5843 - lr: 3.0000e-05 - 103s/epoch - 512ms/step\n",
      "Epoch 10/15\n",
      "202/202 - 104s - loss: 0.9307 - accuracy: 0.6531 - val_loss: 1.0958 - val_accuracy: 0.5855 - lr: 3.0000e-05 - 104s/epoch - 516ms/step\n",
      "Epoch 11/15\n",
      "202/202 - 105s - loss: 0.9102 - accuracy: 0.6647 - val_loss: 1.1055 - val_accuracy: 0.5930 - lr: 3.0000e-05 - 105s/epoch - 518ms/step\n",
      "Epoch 12/15\n",
      "202/202 - 104s - loss: 0.8874 - accuracy: 0.6686 - val_loss: 1.0972 - val_accuracy: 0.5948 - lr: 3.0000e-05 - 104s/epoch - 514ms/step\n",
      "Epoch 13/15\n",
      "202/202 - 103s - loss: 0.8471 - accuracy: 0.6867 - val_loss: 1.0983 - val_accuracy: 0.5942 - lr: 1.5000e-05 - 103s/epoch - 512ms/step\n",
      "Epoch 14/15\n",
      "202/202 - 105s - loss: 0.8285 - accuracy: 0.6902 - val_loss: 1.0974 - val_accuracy: 0.5939 - lr: 1.5000e-05 - 105s/epoch - 520ms/step\n",
      "Epoch 15/15\n",
      "202/202 - 103s - loss: 0.8083 - accuracy: 0.7020 - val_loss: 1.0952 - val_accuracy: 0.5948 - lr: 7.5000e-06 - 103s/epoch - 509ms/step\n"
     ]
    }
   ],
   "source": [
    "base.trainable = True\n",
    "\n",
    "fine_tune_at = int(len(base.layers) * 0.5)  # ŸÅŸÉ 50% ÿßŸÑÿ£ÿÆŸäÿ±ÿ© ÿ®ÿØŸÑ \n",
    "for layer in base.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(3e-5),  # ÿ£ÿµÿ∫ÿ± ŸÖŸÜ 1e-4 ŸÑÿ™ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿßŸÑŸÖŸäÿ≤ÿßÿ™\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,           # ÿ£ŸÉÿ´ÿ± ÿ¥ŸàŸä\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/227 - 64s - loss: 2.1410 - accuracy: 0.1390 - 64s/epoch - 284ms/step\n",
      "Test loss: 2.140988826751709\n",
      "Test accuracy: 0.13899986445903778\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: affectnet_final.keras\n",
      "Saved: class_names.txt\n"
     ]
    }
   ],
   "source": [
    "model.save(\"affectnet_final.keras\")\n",
    "print(\"Saved: affectnet_final.keras\")\n",
    "\n",
    "with open(\"class_names.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in class_names:\n",
    "        f.write(c + \"\\n\")\n",
    "\n",
    "print(\"Saved: class_names.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# ==================================================\n",
    "#                 ÿßŸÑÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑÿπÿßŸÖÿ©\n",
    "# ==================================================\n",
    "MODEL_PATH = \"affectnet_final.keras\"\n",
    "CLASS_NAMES_PATH = \"class_names.txt\"\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# ==================================================\n",
    "#         ÿ™ÿ≠ŸÖŸäŸÑ ŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿπŸÖŸäŸÇ\n",
    "# ==================================================\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "with open(CLASS_NAMES_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# ==================================================\n",
    "#              ÿ•ÿπÿØÿßÿØ Ÿàÿßÿ¨Ÿáÿ© Tkinter\n",
    "# ==================================================\n",
    "root = tk.Tk()\n",
    "root.title(\"AI Emotion Detection System\")\n",
    "root.geometry(\"1000x650\")\n",
    "root.configure(bg=\"#1e1e1e\")\n",
    "\n",
    "# ===== ÿπŸÜŸàÿßŸÜ =====\n",
    "title = tk.Label(root, text=\"Real-Time Emotion Detection\",\n",
    "                 font=(\"Arial\", 20, \"bold\"),\n",
    "                 fg=\"#00d4ff\", bg=\"#1e1e1e\")\n",
    "title.pack(pady=10)\n",
    "\n",
    "# ===== ÿ•ÿ∑ÿßÿ± ÿßŸÑŸÅŸäÿØŸäŸà =====\n",
    "video_frame = tk.Frame(root, bg=\"#1e1e1e\")\n",
    "video_frame.pack()\n",
    "\n",
    "video_label = tk.Label(video_frame)\n",
    "video_label.pack()\n",
    "\n",
    "# ==================================================\n",
    "#              ŸÑŸàÿ≠ÿ© ÿßŸÑÿ™ÿ≠ŸÉŸÖ\n",
    "# ==================================================\n",
    "control_frame = tk.Frame(root, bg=\"#2b2b2b\")\n",
    "control_frame.pack(fill=\"x\", pady=10)\n",
    "\n",
    "brightness_var = tk.IntVar(value=50)\n",
    "blur_var = tk.IntVar(value=1)\n",
    "threshold_var = tk.IntVar(value=0)\n",
    "emotion_var = tk.BooleanVar(value=True)\n",
    "\n",
    "def styled_scale(label_text, variable, from_, to_):\n",
    "    frame = tk.Frame(control_frame, bg=\"#2b2b2b\")\n",
    "    frame.pack(side=\"left\", padx=20)\n",
    "    tk.Label(frame, text=label_text, fg=\"white\",\n",
    "             bg=\"#2b2b2b\").pack()\n",
    "    scale = ttk.Scale(frame, from_=from_, to=to_,\n",
    "                      variable=variable, orient=\"horizontal\", length=150)\n",
    "    scale.pack()\n",
    "\n",
    "styled_scale(\"Brightness\", brightness_var, 0, 100)\n",
    "styled_scale(\"Blur\", blur_var, 0, 20)\n",
    "styled_scale(\"Threshold\", threshold_var, 0, 255)\n",
    "\n",
    "# ==================================================\n",
    "#                 Ÿàÿ∏ÿßÿ¶ŸÅ ÿßŸÑÿ£ÿ≤ÿ±ÿßÿ±\n",
    "# ==================================================\n",
    "def capture_image():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(\"captured_image.png\", frame)\n",
    "        print(\"ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿµŸàÿ±ÿ© ‚úÖ\")\n",
    "\n",
    "def reset_values():\n",
    "    brightness_var.set(50)\n",
    "    blur_var.set(1)\n",
    "    threshold_var.set(0)\n",
    "\n",
    "def toggle_emotion():\n",
    "    emotion_var.set(not emotion_var.get())\n",
    "\n",
    "def exit_app():\n",
    "    cap.release()\n",
    "    root.destroy()\n",
    "\n",
    "button_frame = tk.Frame(root, bg=\"#1e1e1e\")\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "tk.Button(button_frame, text=\"üì∏ Capture\",\n",
    "          command=capture_image,\n",
    "          bg=\"#4CAF50\", fg=\"white\",\n",
    "          width=12).pack(side=\"left\", padx=10)\n",
    "\n",
    "tk.Button(button_frame, text=\"üîÑ Reset\",\n",
    "          command=reset_values,\n",
    "          bg=\"#ff9800\", fg=\"white\",\n",
    "          width=12).pack(side=\"left\", padx=10)\n",
    "\n",
    "tk.Button(button_frame, text=\"üß† Emotion ON/OFF\",\n",
    "          command=toggle_emotion,\n",
    "          bg=\"#2196F3\", fg=\"white\",\n",
    "          width=15).pack(side=\"left\", padx=10)\n",
    "\n",
    "tk.Button(button_frame, text=\"‚ùå Exit\",\n",
    "          command=exit_app,\n",
    "          bg=\"#f44336\", fg=\"white\",\n",
    "          width=12).pack(side=\"left\", padx=10)\n",
    "\n",
    "# ==================================================\n",
    "#              ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÅŸäÿØŸäŸà\n",
    "# ==================================================\n",
    "def update_frame():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    processed = frame.copy()\n",
    "\n",
    "    # ===== ÿ™ÿ≠ÿ≥ŸäŸÜ ÿ•ÿ∂ÿßÿ°ÿ© ÿ™ŸÑŸÇÿßÿ¶Ÿä (CLAHE) =====\n",
    "    lab = cv2.cvtColor(processed, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l,a,b))\n",
    "    processed = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    # ===== ÿ•ÿ∂ÿßÿ°ÿ© ŸäÿØŸàŸäÿ© =====\n",
    "    beta = brightness_var.get() - 50\n",
    "    processed = cv2.convertScaleAbs(processed, alpha=1.0, beta=beta)\n",
    "\n",
    "    # ===== Blur =====\n",
    "    blur_value = int(blur_var.get())\n",
    "    if blur_value > 0:\n",
    "        if blur_value % 2 == 0:\n",
    "            blur_value += 1\n",
    "        processed = cv2.GaussianBlur(processed, (blur_value, blur_value), 0)\n",
    "\n",
    "    # ===== Threshold =====\n",
    "    if threshold_var.get() > 0:\n",
    "        gray_temp = cv2.cvtColor(processed, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh_img = cv2.threshold(\n",
    "            gray_temp, threshold_var.get(), 255, cv2.THRESH_BINARY)\n",
    "        processed = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # ===== ŸÉÿ¥ŸÅ ÿßŸÑŸàÿ¨Ÿá =====\n",
    "    gray = cv2.cvtColor(processed, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if emotion_var.get():\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = processed[y:y+h, x:x+w]\n",
    "            cv2.rectangle(processed, (x, y),\n",
    "                          (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "            face_resized = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "            face_array = face_resized.astype(\"float32\") / 255.0\n",
    "            face_array = np.expand_dims(face_array, axis=0)\n",
    "\n",
    "            prediction = model.predict(face_array, verbose=0)[0]\n",
    "            best_index = np.argmax(prediction)\n",
    "            emotion = class_names[best_index]\n",
    "            confidence = prediction[best_index] * 100\n",
    "\n",
    "            cv2.putText(processed,\n",
    "                        f\"{emotion} ({confidence:.1f}%)\",\n",
    "                        (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7, (0,255,0), 2)\n",
    "\n",
    "    # ===== ÿπÿ±ÿ∂ ÿØÿßÿÆŸÑ Tk =====\n",
    "    img_rgb = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img_rgb)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    video_label.imgtk = imgtk\n",
    "    video_label.configure(image=imgtk)\n",
    "\n",
    "    root.after(10, update_frame)\n",
    "\n",
    "update_frame()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp_a6jpfus\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp_a6jpfus\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: emotion_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "MODEL_PATH = \"affectnet_final.keras\"\n",
    "OUT_PATH = \"emotion_model.tflite\"\n",
    "\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(OUT_PATH, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Saved:\", OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('fer_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e9f2ab6f4954a9eb4576e99f52a93db635088538cd90d4aedf507e1fcf9fe4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
